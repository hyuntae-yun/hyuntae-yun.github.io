---
layout: post
title:  "Liquid Time-constant Networks-intro"
date:   2021-11-02 22:01:00 +0300
image:  tliquidnetwork.png
tags:   Liquid Time-constant Networks,review
---


데이터의 중요성 때문에 최근에 이상치 탐지에 대해 많은 관심을 가지고 있었지만, 최근 유튜브를 보던 중

제법 흥미로운 주제가 있어서 리뷰를 시작하게 되었습니다.

MITCBMM에서 Liquid Neural Networks(https://www.youtube.com/watch?v=IlliqYiRhMU&t=2024s)를 보게

되었는데, 아마도 많은 사람들이 가장 관심있어 하는 '인간의 뇌를 모방하여 만든 모델로 문제를 해결하기'를

실제로 한 게 아닐까 싶습니다. 관심도 높아서 오늘자 기준으로 (2021년 11월 2일) 벌써 10만 뷰를 달성했습니다.


## 1. 어떤 식으로 뇌를 모방했을까?


논문을 발표하는 Ramin Hasani씨는 유튜브에서 다음과 같이 말합니다.

"실제 두뇌가 어떻게 조직에 접근하는 지에 대한 관심은 지금까지 많이 있었습니다. AI의 궁극적인 목표는 

인간을 닮아가는 것이기 때문이죠. 그래서 실제로 어떻게 세상과 상호작용을 하는지를 잘 이해해야 좋은

딥러닝 모델을 설계할 수 있습니다. 그리고 우리 뇌는 robust(이상치에 영향을 적게 받고) 하면서도 

굉장히 유연한 사고방식을 가지고 있습니다."

이렇게 설명하면서 영상을 보여주는데, 컴퓨터 비젼을 예로 들고 있습니다. 

컴퓨터 비젼은 CNN(Convolutional Neural Network)을 사용하는데, 이 때 fully connected layer중에서 

어떤 부분이 활성화 되는지에 따라서 모델의 예측이 달라집니다. 

CNN의 ATTention Map을 보시면, 길이 아니라 길의 주변 부에 집중을 하고 있는 것을 볼 수 있습니다. 그건

길을 찾아야 하는 모델의 목적과 전혀 상반된 곳을 집중한다는 의미입니다. 만약에 인간이 동일한 화면을 본다면,

인간의 뇌는 주변부에 집중하기 보다는 길 그 자체에 집중을 할 것입니다. 

그래서 만약에 이렇게 주변 부에 집중을 하게 된다면, 조금의 노이즈를 주는 것 만으로도 CNN 모델은 혼동을 

일으킵니다.


그렇다면 이런 현상을 개선 시키려면 어떻게 해야할까요? 저자는 다음과 같이 말합니다.

"우선 Marr 과 Poggio가 말한 것처럼 실제로 생물학적 시스템을 만들기 위한 틀을 세웁니다. 
 
 예를 들어, 시스템 수준에서 그것을 보고 시스템의 목표가 무엇인지, 실제로 목표를 달성하는 메커니즘이 
 
 무엇인지를 파악합니다.

그리고 우리들은 이러한 것들을 구성하는 블록을 관찰하고, 아래로 내려가서 어떻게 지능이 세포에서 나오는지를 

관찰하는 관점을 가질 수 있습니다. 더 나아가서 기본적으로 생물학에 존재하는 정확한 메커니즘인 계산 모델을 

사용할 수 있습니다."

즉, 인간의 뇌를 모방한 생물학적 구조를 딥러닝 모델로 구현해서 실현 시키겠다는 이야기 입니다. 인간의 뇌하면

무엇이 떠오르십니까? 바로 신경망 입니다. 뇌는 약 1000억개의 신경세포들로 이루어진 복잡한 신경망입니다. 

 그리고 논문의 저자는 그 신경망들을 이루는 최소 단위, 즉 시냅스를 어떻게 만들어 낼 것 인지 고민했습니다.
 
아니 사실은 더 깊이 내려가고 싶었지만 저자는 만족할 수준 까지만 내려갔다고 했습니다. 앞으로 차차 설명하겠지만,

그렇게 완성된 뉴런 모델은 언뜻 보기에는 간단한 추상화 과정을 거쳤지만, 명시적이거나 암묵적인 메모리 메커니즘을

가졌고, 가장 중요한 것은 데이터의 진정한 인과 구조를 포착할 수 있다는 것 입니다. 즉, 길을 찾는 모델을 만들고자

한다면 모델은 이를 기억하고 길의 주변부가 아니라 길에만 집중하는 모델이 생성된다는 것입니다. 그리고 가장 중요한

것은, 어떤 프로세스가 데이터의 인과 구조를 포착할 수 있고 그것이 사실임을 증명하기만 한다면 시스템이 분포를

벗어나 올바른 방향으로 학습이 진행될 수 있기 때문입니다.

 지금까지는 Liquid Neural Network의 가능성에 대해서 알아보았습니다. 그리고 지금부터는, 어떻게 이 모델을
 
 만들었는지에 대한 설명이 이어집니다.
 
 ## 2. 어떤 식으로 모델을 만들었을까?

 저자는 가장 먼저, 뉴런에 집중했습니다. 뉴런이 정보를 받아들이는 과정은 다른 축삭 돌기의 말단들로 부터 얻은
 
 정보들을 수상 돌기들로 부터 취합해, 자체적으로 역치를 계산한 다음, 활동 전위보다 높아지면 다음 신호를
 
 내보내고, 아니면 신호를 내보내지 않는 방식으로 작동합니다. 또한 모든 신호 전파는 연속적인 형태로
 
 이루어지기 때문에 미분으로 정의할 수 있습니다. 그리고 그 값들이 모이면, 자극점을 넘어 신호를 보낼 지 말지를
 
 다른 방식으로 정의하기 때문에 비선형적인 형태(이를 테면 sigmoid 함수)를 띄게 됩니다. 또한, 실제로 모델상으로는
 
 뉴런들이 연결되어 있지만, 활동 여부는 각각의 함수 계산에 달려있어 실제로는 뉴런들이 듬성듬성(sparsity) 활동 
 
 하는 것을 관찰할 수 있습니다. 
 
  자, 여기까지 말로만 들으면 굉장히 그럴듯해보이고, 쉬워보입니다. 그러면 조금 더 학문적인 부분을 파고들어가 어떻게
  
  모델을 구현했는지 살펴보겠습니다. 우선 모델은 다음과 같은 기존 모델의 성능을 향상시키는 방향으로 설계되었습니다.
  
 1. 표현 학습 개선
 2. 모델의 견고성과 유연성 향상
 3. 모형의 해석 능력 향상


 위의 목표를 달성 시키기 위해, 저자는 continuous-time/depth neural network를 제시했습니다.
 
 ##3. continuous-time/depth neural network는 무엇일까?
 

![ctd](https://user-images.githubusercontent.com/70379885/155934175-383ec54e-d062-4ca5-a8c2-56eb95831432.jpg)


[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/

